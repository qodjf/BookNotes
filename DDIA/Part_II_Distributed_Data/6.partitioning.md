# 分区
上一章讨论了复制技术，即在不同节点上保存相同数据的多个副本。然而，如果数据的总大小超过单个节点所能承载的上限，只使用复制技术是不够的。我们需要把数据分配给多个节点处理。这种技术称作分区。

分区通常与复制结合使用，即所有数据分布在多个节点上，而每个数据又在多个节点上有副本。分区使得系统可扩展，复制使得系统可容错。

## 引入
如何分配？

**目标**是把负载均匀地分布在所有节点上。则系统可以线性水平扩展，这是最理想最优的可扩展性。而如果分布不均匀（称之为倾斜），就可能出现“一核有难，9 核围观”的现象，无法发挥出所有节点的全部性能。

负载包括计算（读写请求）和存储（数据）上的负载。先看如何使数据均匀分布，后面再考虑即使数据均匀分布，读写请求仍不均匀的问题。

最简单的办法是将记录随机分配给所有节点。这种方法确实很均匀，然而缺点是读取特定 key 时，无法知道它在哪个节点上，所以不得不并行查询所有节点。因此，一定要有一个规则去决定 key 在哪个节点上（即一个映射），然后去查询对应的节点。

如何把 key 映射到节点？定义域是所有 key 组成的集合，值域是 n 个节点。

如果 key 是整数，则一个自然而然的想法是 f(key) = key % n。这种策略的缺点是，如果 key % n 很可能不均匀取值。改进办法是 f(key) = hash(key) % n。hash 使得可以处理不为整数的 key，同时它使得 f(key) 均匀分布。这种策略的问题是，如果节点数发生了变化，会导致很多数据需要迁移。随着 n 的变化，key 到分区的映射非常不稳定。为避免影响正常运行的系统，我们需要尽可能减少需要迁移的数据。

如何使得所需迁移的数据量最少？假设有 n 个节点，每个节点有 $\frac{1}{n}$ 的数据，现在添加一个节点，为了分布均匀，每个节点需要保留 $\frac{1}/{n+1}$ 的数据。因此，每个节点需要迁走 $\frac{1}/{n*(n+1)}$ 的数据。不妨把所有数据划分为 $n*(n+1)$ 份，每个节点上存放其中的 $n+1$ 份，新增一个节点时，原来的每个节点拿出一份交给这个新的节点，则达到了数据在节点之间分布均匀且迁移数据量最少的目的。把这里的一份数据称作一个分区，也就是说分区是迁移的最小单位。

现在，我们可以大致窥探到分区技术的面貌。通常是这样的：每一条数据只属于一个分区，每个节点上可以有多个分区。实际上，每个分区可以被视为一个完整的小型数据库，虽然数据库可能存在一些跨分区的操作。而节点之间数据均匀划分依靠的是：每个分区里的数据量相等，每个节点上的分区数相等。接下来的 KV 数据的分区讲述的即是如何使每个分区里的数据量相等，而后面的分区再平衡则讲述的是分区在节点之间迁移以使得各个节点上的分区数相等。

## KV 数据的分区
### 根据 key 的范围分区
#### 实现
一种分区方式是把 key 的范围切分为一个个更小的区间，每个分区对应一个区间。根据 key 可以知道它属于哪个分区，如果还知道分区属于哪个节点，就可以把请求发送到对应节点上。

区间的切分不一定要均匀，这是因为数据本身可能就不均匀。例如，以 A 开头的单词很多，以 Z 开头的单词却只有几页。为了更均匀地分布数据，分区边界需要适配数据本身的分布特征。此外，[动态调整分区边界](#动态调整分区边界)也非常有助于各分区的数据均匀。

分区边界可以由管理员手动指定，也可以由数据库自动选择。采用这种分区策略的系统包括 Bigtable，其开源实现 HBase，RethinkDB 和 2.4 版本之前的 MongoDB。

#### 优点：支持区间查询
如果每个分区内按照 key 排序保存（例如 LSM-Tree），则这种分区方式可以轻松支持区间查询。原因很简单，按区间的顺序把所有区间合到一起，则得到了按顺序排列的所有数据。可以认为 key 是一个多列索引，例如如果 key 是年月日，则我们可以按年的区间查询，也可以按年月的区间查询。

#### 缺点：可能导致热点
然而，其缺点是某些访问模式会导致热点。例如，如果 key 是时间戳，则分区对应一个时间范围，例如每天一个分区。则每天的写入操作都只对应一个分区，而其它分区则处于空闲状态。这就是数据分布均匀时读写分布不均匀的例子。

为了解决上述问题，需要使用时间戳以外的其它内容（例如随机数）作为关键字的第一项，后面再接上时间戳。

### 根据 key 的 hash 分区
除了强迫用户更改 key 以外，还可以通过基于 key 的 hash 值分区解决上述热点问题。

一个好的哈希函数可以使数据均匀分布。即使输入的字符串非常相似，返回的哈希值也会在总的范围内均匀分布。

一旦找到合适的哈希函数，就可以为每个分区分配一个 hash 值的范围。都是范围，然而这里是哈希值的范围，而不是 key 的范围。
#### 缺点：区间查询不好
然而，通过关键字哈希进行分区，我们丧失了良好的区间查询特性。即使在单个分区内有序，仍然需要把读请求发给所有的分区。在基于哈希的分区模式的 MongoDB 中就是如此。而 Riak、Couchbase 和 Voldemort 干脆就不支持区间查询。

Cassandra 则在两种策略中做了一个折中。表需要声明多个列作为复合主键。复合主键的第一部分用于支持哈希分区，而其它列则作为组合索引来对 sstable 中的数据进行排序。因此，只要第一列指定好了固定值，就可以对其它列执行高效的区间查询。

这种组合索引为一对多的关系提供了一个优雅的数据模型。例如在社交网站上，一个用户可能会发布很多消息。可以把关键字设置为 (user_id, timestamp)，则可以高效检索某一固定用户在一段时间内的所有消息。

### 负载倾斜和热点问题
基于哈希的分区方法可以减轻热点，但无法完全避免。一个极端情况是，所有的请求都访问同一个关键字，则所有请求都被路由到同一个分区。例如，社交网站上的某个明星发布了一条热点消息，就可能引发一场访问风暴。这是另一个数据分布均匀时读写分布不均匀的例子。

对于这种高度倾斜的负载，今天的大多数系统仍然无法自动消除，而只能通过应用层来减轻倾斜程度。例如，一个**简单的技术**是在热点关键字的开头或结尾添加一个随机数，使得写请求被分配给多个分区。随之而来的是，查询时，需要从所有分区中读取数据然后进行合并。也许将来可以自动检测负载倾斜情况并自动处理。

## 二级索引的分区
上一节讲了 kv 数据的分区，kv 数据就是主键索引。除了主键索引以外，还有二级索引。二级索引就是以其它列为 key 的索引。二级索引通常不能唯一标识一条记录，而是用来加速特定值的查询。

二级索引是关系数据库的必备特性，在文档数据库中应用也非常普遍。考虑到其复杂性，HBase 和Voldemort 不支持；但在 Riak 中开始增加对二级索引的支持。而对于 Solr 和 Elasticsearch 等全文索引服务器，二级索引是其存在之根本。

二级索引带来的**主要挑战**是：它们不能整洁地映射到分区中。有两种主要的方法来对二级索引分区：基于文档的分区和基于词条的分区。简单来说，基于文档，就是仍然按照文档的主键来分区；而基于词条，就是按照二级索引的键来分区。

### partitioning by document
基于文档的分区就是在原来的分区内添加上二级索引的支持。在这种索引方法中，每个分区完全独立，各自维护自己的二级索引，且只负责自己分区内的文档。因此文档分区也被称为本地索引，而不是全局索引。

但读取时需要注意，由于分区是按照主键做的分区，所以通过二级索引查询时，数据不太可能都在一个分区中。因此读取时需要将查询发送到所有的分区，然后合并返回的结果。这种查询方法有时也被称为 scatter/gather。显然这种查询代价高昂。即使采用并行查询，也容易显著增加读延迟。

### partitioning by term
另一种方式是，我们可以对所有的数据构建全局索引，而不是每个分区维护自己的本地索引。显然，全局索引本身也需要进行分区，且可以与 kv 索引采用不同的分区策略（range-based 还是 hash-based）。

**优点**：相比于文档分区索引，全局索引的读取更为高效。客户端只需要向包含词条的那一个分区发出读请求，然后再去访问包含所需文档的节点。

**缺点**：写入速度慢且非常复杂。主要是因为单个文档更新时，里面可能涉及到多个二级索引，而二级索引的分区又极可能在不同的分区乃至不同的节点上，引入了显著的写放大。此外，如果希望写入的数据立刻反映在索引上，则需要跨分区的分布式事务支持，写入速度会受到极大影响。所以现有数据库都不支持同步更新二级索引。

## 分区再平衡
随着时间推移，数据库可能会出现某些变化：
* 查询压力增加
* 数据规模增加
* 节点出现故障

这些变化都要求数据和请求可以从一个节点转移到另一个节点。这样一个迁移负载的过程称为再平衡。无论哪种分区方案，分区再平衡都通常要满足：
* 平衡后，负载（数据存储和读写请求）应该更均匀地分布
* 平衡过程中，数据库可以正常提供读写服务
* 避免不必要的负载迁移，以加快再平衡并尽量减少对网络和磁盘 IO 的影响

### 再平衡的策略
将分区对应到节点有多种策略。
#### 固定总的分区数量
第一种策略很简单。首先，创建远超实际节点数的分区数，然后每个节点分配多个分区。

接下来，如果集群中添加了一个节点，该新节点可以从每个现有的节点上匀走几个分区，以达到全局平衡。删除节点则相反。节点数变化时，唯一要调整的是分区和节点的对应关系。这样的调整可以逐步完成，在此期间，旧的分区仍然可以接收读写请求。

使用该策略时，分区的数量在数据库创建时就确定好，之后不会改变。分区数太小则无法分布均匀，且由于一个分区内的数据量更大，在迁移时的代价就更大。而分区数太大则会带来额外的管理开销。需要谨慎选择。如果数据集的总规模高度不确定，此时选择合适的分区数就有些困难。

#### 固定每个节点上的分区数量
Cassandra 和 Ketama 采用了第二种方式，使总分区数和总节点数成正比，也即每个节点具有固定数量的分区。较大的数据量通常需要大量的节点来存储，因此，这种方法也使每个分区的大小保持稳定。

当一个新节点加入集群时，它随机选择固定数量的现有分区进行分裂，然后拿走一半，将另一半留在原节点。并且随机选择分裂的间隔点。在单个分区上这很可能带来不公平的分裂，但是当在大量的分区上平均后，从每个节点上迁移的负载是公平的。

随机选择分区边界的前提是使用 hash-based 的分区策略。为什么呢？

#### 动态调整分区边界
对于 range-based 的关键字分区策略，如果边界设置有问题，最终可能发生所有数据都挤在一个分区而其它分区基本为空。因此，需要动态调整分区的边界。

当分区的数据超过某个阈值，就将其分裂为两个分区。相反，如果两个分区数据减少到某个阈值，就将其合并。类似于 B 树的分裂与合并。

动态调整分区边界的主要目的是使得各个分区之间的数据大小是均匀的，大小都固定了当然就均匀了。而基于 hash 的分区策略不存在这个问题。但是基于哈希的分区策略也可以使用动态分区，以使得每个分区内数据量固定。注意，hash-based 其实是 hash range based。

分区的数目会随着分裂或合并而加 1或减 1。分裂时尽量把多出来的那个分区扔到分区数最少的节点，合并时尽量使得分区数较大的那个节点上的分区数减 1。

#### 小结
回顾一下这三种策略。(todo)

节点数 * 每个节点上的分区数 * 每个分区里的数据量 = 数据总量。

### 运维：自动与手动再平衡
动态平衡的另一个重要问题是：它是自动执行还是手动执行？

全自动与纯手动各有优缺点。之间，还有一个过渡阶段，半自动。即系统自动生成一个再平衡的方案，但需要管理员确认后才能生效。

将自动平衡与自动故障检测结合也可能存在风险。例如，假设某个节点过载，无法处理请求，此时可能判定其已经失效；接下来自动平衡会转移其负载。客观上这会加重该节点、其它节点以及网络的负载，可能使总体情况变得更糟，甚至导致级联式的扩散。

因此，让管理员介入可能是更好的选择。它比全自动响应慢一些，但是可以有效防止意外发生。

## 请求路由

## QA
Q: 迁移过程中如何继续提供读写服务？

Q: 路由信息过期，请求被路由到了已经将分区迁走的节点怎么办？

Q: 既然 range-based 的分区可以通过动态调整分区边界来使得每个分区的数据量均匀，那么 hash-based 还有什么用呢？
A: 避免特定访问模式造成热点

key 均匀地映射到分区，分区均匀地映射到节点。每个节点的分区数量要相同，可以是固定不变，也可以是都发生相同的变化。每个分区的数据量要相同，可以通过动态调整分区边界，也可以通过 hash-based 方法。

Q: 分区数量增加后应该怎样调整分区的边界？
